{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNCUMClVMAl2"
      },
      "source": [
        "# Train Neural Speech Decoding on Google Colab\n",
        "\n",
        "**Requirements:** Colab Pro (for 24hr sessions + better GPU)\n",
        "\n",
        "**Total Time:** ~16 hours (6hrs Stage 1 + 10hrs Stage 2 on A100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GitHub Push Code"
      ],
      "metadata": {
        "id": "3Hq9suwSRkYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git add colab_training.ipynb\n",
        "!git commit -m \"update notebook\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGBxAX5HSlPc",
        "outputId": "023f8940-2fd8-4ee8-d573-563799ec313a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"update\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfPqyYWrRy4i",
        "outputId": "92035264-c193-425b-8494-630619337ef4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main --rebase\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw0SZ8IaS4Wr",
        "outputId": "162306f8-11af-4841-a0be-37c22a804610"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"maidouatomwang@gmail.com\"\n",
        "!git config --global user.name \"atomiiw\"\n",
        "!git remote set-url origin https://atomiiw:<TOKEN>@github.com/atomiiw/neural_speech_decoding.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07E6pO5KRoNt",
        "outputId": "4143622f-4004-46ae-ea59-010173f196e8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: TOKEN: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yai__P_TMAl2"
      },
      "source": [
        "## Step 1: Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAhHEVltMAl2",
        "outputId": "2e699055-0bea-4097-e4c3-51eb6fc90781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov  7 02:41:21 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arxXMr4aMAl3"
      },
      "source": [
        "## Step 2: Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7H7T8_YMAl3",
        "outputId": "1168a968-c8c5-44cc-93f5-9ba438b39482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'neural_speech_decoding' already exists and is not an empty directory.\n",
            "/content/neural_speech_decoding\n",
            "/content/neural_speech_decoding\n"
          ]
        }
      ],
      "source": [
        "# Clone the repo into /content\n",
        "%cd /content\n",
        "!git clone https://github.com/flinkerlab/neural_speech_decoding.git\n",
        "\n",
        "# Enter the repo - this is our workspace\n",
        "%cd neural_speech_decoding\n",
        "\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "%cd neural_speech_decoding"
      ],
      "metadata": {
        "id": "L8OLyaYmiSpe",
        "outputId": "251544f2-7d69-4f80-8ff2-0fac3868d82c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/neural_speech_decoding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPw1xRzQMAl3"
      },
      "source": [
        "## Step 3: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yacs h5py librosa scipy soundfile pesq pystoi tqdm matplotlib seaborn -q"
      ],
      "metadata": {
        "id": "j6gRKLBrMYhS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHPENoCQMAl3"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4nOPRDUMAl3"
      },
      "source": [
        "## Step 4: Mount Google Drive & Setup Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qulz_1fLMAl3",
        "outputId": "a990c857-c6a9-42a4-c3cd-23e296284129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted and linked\n",
            "  Data: example_data/data -> Google Drive\n",
            "  Output: output -> Google Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create persistent storage in Google Drive\n",
        "!mkdir -p /content/drive/MyDrive/nsd_data\n",
        "!mkdir -p /content/drive/MyDrive/nsd_outputs\n",
        "\n",
        "# Link them to the repo workspace\n",
        "!mkdir -p example_data\n",
        "!ln -s /content/drive/MyDrive/nsd_data example_data/data\n",
        "!ln -s /content/drive/MyDrive/nsd_outputs output\n",
        "\n",
        "print(\"✓ Google Drive mounted and linked\")\n",
        "print(f\"  Data: example_data/data -> Google Drive\")\n",
        "print(f\"  Output: output -> Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQp3th7PMAl3"
      },
      "source": [
        "## Step 5: Upload Data to Google Drive\n",
        "\n",
        "**Before running training, you need to:**\n",
        "\n",
        "1. Download HB02 dataset from: https://data.mendeley.com/datasets/fp4bv9gtwk/2\n",
        "2. Upload the files to: **MyDrive/nsd_data/** in your Google Drive\n",
        "3. Verify they're there by running the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmkXvLxjMAl3",
        "outputId": "9cd1cb06-a826-4126-edbc-24a6c915a91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1.1G\n",
            "-rw------- 1 root root 1.1G Nov  7 03:11 HB02.h5\n",
            "\n",
            "If empty, please upload HB02 data to: MyDrive/nsd_data/ in Google Drive\n"
          ]
        }
      ],
      "source": [
        "# Check if data is present\n",
        "!ls -lh /content/drive/MyDrive/nsd_data/\n",
        "\n",
        "# Should see HB02 data files (*.hdf5 or *.h5)\n",
        "print(\"\\nIf empty, please upload HB02 data to: MyDrive/nsd_data/ in Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY4MzeDuMAl3"
      },
      "source": [
        "## Step 6: Update Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kB1hxvmMAl3",
        "outputId": "8f4d9e43-81be-42b5-cbd7-704433f83ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Config updated: RootPath = ./example_data/data/\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Update data path in config\n",
        "with open('configs/AllSubjectInfo.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "config['Shared']['RootPath'] = './example_data/data/'\n",
        "\n",
        "with open('configs/AllSubjectInfo.json', 'w') as f:\n",
        "    json.dump(config, f, indent=4)\n",
        "\n",
        "print(f\"✓ Config updated: RootPath = {config['Shared']['RootPath']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRy8us5xMAl3"
      },
      "source": [
        "## Step 7: Stage 1 - Audio-to-Audio Training (a2a)\n",
        "\n",
        "**Time:** ~6 hours on A100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWpZzLjRMAl3",
        "outputId": "c13f7a26-3d70-482e-a1d7-12d64cac43b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['HB02']\n",
            "rank in _run 0\n",
            "2025-11-07 04:34:03,786 logger INFO: Namespace(config_file='configs/a2a_production.yaml', opts=[], subject='NYxxx', trainsubject='HB02', testsubject='HB02', DENSITY='HB', OUTPUT_DIR='output/a2a/HB02', wavebased=1, bgnoise_fromdata=1, ignore_loading=0, finetune=0, learnedmask=0, dynamicfiltershape=0, formant_supervision=1, pitch_supervision=0, intensity_supervision=0, n_filter_samples=80, n_fft=256, reverse_order=1, lar_cap=0, intensity_thres=-1.0, unified=0, ONEDCONFIRST=1, RNN_TYPE='LSTM', RNN_LAYERS=1, RNN_COMPUTE_DB_LOUDNESS=1, BIDIRECTION=1, MAPPING_FROM_ECOG='ECoGMappingBottleneck_ran', COMPONENTKEY='', old_formant_file=0, reshape=1, fastattentype='full', phone_weight=0, ld_loss_weight=1, alpha_loss_weight=1, consonant_loss_weight=0, amp_formant_loss_weight=0, component_regression=0, freq_single_formant_loss_weight=0, amp_minmax=0, amp_energy=0, f0_midi=0, alpha_db=0, network_db=0, delta_time=0, delta_freq=0, cumsum=0, distill=0, noise_db=-50, classic_pe=0, temporal_down_before=0, classic_attention=1, batch_size=16, param_file='configs/a2a_production.yaml', pretrained_model_dir='', causal=0, anticausal=0, rdropout=0, epoch_num=60, use_stoi=0, use_denoise=0)\n",
            "2025-11-07 04:34:03,786 logger INFO: World size: 1\n",
            "2025-11-07 04:34:03,786 logger INFO: Loaded configuration file configs/a2a_production.yaml\n",
            "Running on  NVIDIA A100-SXM4-40GB\n",
            "within train function 256\n",
            "patient in model HB02\n",
            "encoder loudness, 256\n",
            "self.noise_db,self.max_db -50 22.5\n",
            "patient in model HB02\n",
            "encoder loudness, 256\n",
            "self.noise_db,self.max_db -50 22.5\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/neural_speech_decoding/train_a2a.py\", line 940, in <module>\n",
            "    run(\n",
            "  File \"/content/neural_speech_decoding/utils/launcher.py\", line 174, in run\n",
            "    _run(0, world_size, fn, defaults, write_log, no_cuda, args_)\n",
            "  File \"/content/neural_speech_decoding/utils/launcher.py\", line 158, in _run\n",
            "    fn(**matching_args_)\n",
            "  File \"/content/neural_speech_decoding/train_a2a.py\", line 610, in train\n",
            "    sample_dict_test = next(iter(dataset_test_all[subject].iterator))\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 494, in __iter__\n",
            "    return self._get_iterator()\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 424, in _get_iterator\n",
            "    return _SingleProcessDataLoaderIter(self)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 766, in __init__\n",
            "    super().__init__(loader)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 699, in __init__\n",
            "    .random_(generator=loader.generator)\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Expected a 'cuda' device type for generator but found 'cpu'\n"
          ]
        }
      ],
      "source": [
        "!python train_a2a.py \\\n",
        "  --OUTPUT_DIR output/a2a/HB02 \\\n",
        "  --trainsubject HB02 \\\n",
        "  --testsubject HB02 \\\n",
        "  --param_file configs/a2a_production.yaml \\\n",
        "  --batch_size 16 \\\n",
        "  --reshape 1 \\\n",
        "  --DENSITY \"HB\" \\\n",
        "  --wavebased 1 \\\n",
        "  --n_filter_samples 80 \\\n",
        "  --n_fft 256 \\\n",
        "  --formant_supervision 1 \\\n",
        "  --intensity_thres -1 \\\n",
        "  --epoch_num 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6izAitkqMAl3"
      },
      "outputs": [],
      "source": [
        "# Check Stage 1 completed\n",
        "!ls output/a2a/HB02/*.pth | wc -l\n",
        "print(\"Expected: 60 checkpoint files (model_epoch0.pth to model_epoch59.pth)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuNpS19mMAl3"
      },
      "source": [
        "## Step 8: Stage 2 - ECoG-to-Audio Training (e2a)\n",
        "\n",
        "**Time:** ~10 hours on A100\n",
        "\n",
        "**This produces the weights you need for phoneme classification!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjdpF4fnMAl3"
      },
      "outputs": [],
      "source": [
        "!python train_e2a.py \\\n",
        "  --OUTPUT_DIR output/e2a/resnet_HB02 \\\n",
        "  --trainsubject HB02 \\\n",
        "  --testsubject HB02 \\\n",
        "  --param_file configs/e2a_production.yaml \\\n",
        "  --batch_size 16 \\\n",
        "  --MAPPING_FROM_ECOG ECoGMapping_ResNet \\\n",
        "  --reshape 1 \\\n",
        "  --DENSITY \"HB\" \\\n",
        "  --wavebased 1 \\\n",
        "  --dynamicfiltershape 0 \\\n",
        "  --n_filter_samples 80 \\\n",
        "  --n_fft 256 \\\n",
        "  --formant_supervision 1 \\\n",
        "  --intensity_thres -1 \\\n",
        "  --epoch_num 60 \\\n",
        "  --pretrained_model_dir output/a2a/HB02 \\\n",
        "  --causal 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G906_Q6WMAl4"
      },
      "outputs": [],
      "source": [
        "# Check Stage 2 completed\n",
        "!ls output/e2a/resnet_HB02/*.pth | wc -l\n",
        "!ls -lh output/e2a/resnet_HB02/model_epoch59.pth\n",
        "\n",
        "print(\"\\n✓✓✓ TRAINING COMPLETE ✓✓✓\")\n",
        "print(\"\\nYour pretrained weights:\")\n",
        "print(\"  output/e2a/resnet_HB02/model_epoch59.pth\")\n",
        "print(\"\\nAlso saved to Google Drive:\")\n",
        "print(\"  /content/drive/MyDrive/nsd_outputs/e2a/resnet_HB02/model_epoch59.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT6ahMQtMAl4"
      },
      "source": [
        "## Step 9: Download Weights (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6x1SvlNMAl4"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Uncomment to download the final checkpoint to your computer:\n",
        "# files.download('output/e2a/resnet_HB02/model_epoch59.pth')\n",
        "\n",
        "print(\"Weights are in Google Drive at: MyDrive/nsd_outputs/e2a/resnet_HB02/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Ewd032MAl4"
      },
      "source": [
        "## Next Steps: Use for Phoneme Classification\n",
        "\n",
        "Update your `ecog_decoder_finetune.ipynb` with:\n",
        "\n",
        "```python\n",
        "checkpoint_path = \"output/e2a/resnet_HB02/model_epoch59.pth\"\n",
        "# Or from Google Drive:\n",
        "# checkpoint_path = \"/content/drive/MyDrive/nsd_outputs/e2a/resnet_HB02/model_epoch59.pth\"\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}